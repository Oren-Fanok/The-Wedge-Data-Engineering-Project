{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1cace2",
   "metadata": {},
   "source": [
    "# Read, Clean, Upload Wedge Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3341407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import janitor\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8897f7",
   "metadata": {},
   "source": [
    "# Connect to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ad48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"C:\\\\Users\\\\ofano\\\\Documents\\\\MSBA Folder\\\\MSBA Folder\\\\\"\n",
    "service_file = 'msba-project-2022-75bb8251ef6f.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'msba-project-2022' # change this to your project. \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9e6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ffbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdc7740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msba-project-2022:dram_shop\n",
      "msba-project-2022:wedge_dataset\n"
     ]
    }
   ],
   "source": [
    "for item in client.list_datasets() : \n",
    "    print(item.full_dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bbadc8",
   "metadata": {},
   "source": [
    "# Specify Google Big Query location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d5da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_pattern = re.compile(r\"(\\D{12})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844f3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"wedge_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e7892b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at transArchive_201001_201003\n",
      "Looking at transArchive_201004_201006\n",
      "Looking at transArchive_201007_201009\n"
     ]
    }
   ],
   "source": [
    "tables = client.list_tables(dataset_id)  \n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    print(f'Looking at {table.table_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46b49ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted transArchive_201001_201003.\n",
      "Deleted transArchive_201004_201006.\n",
      "Deleted transArchive_201007_201009.\n"
     ]
    }
   ],
   "source": [
    "#the next two lines create my table id in gbq format\n",
    "\n",
    "#this line delete my table out of gbq if it exists\n",
    "for table in client.list_tables(dataset_id) :\n",
    "    if name_pattern.search(table.table_id) : \n",
    "        table_id = \".\".join([gbq_proj_id,dataset_id,table.table_id])\n",
    "        client.delete_table(table_id, not_found_ok=True)\n",
    "\n",
    "        print(f\"Deleted {table.table_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5176df3e",
   "metadata": {},
   "source": [
    "# Reading in and Cleaning the zips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c55ce0",
   "metadata": {},
   "source": [
    "Now we need to read in our dirty zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "032c2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_files = os.listdir(\"Wedge_zips\\\\\")\n",
    "#zip_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e5636",
   "metadata": {},
   "source": [
    "Now let's open the files, find their delimiters and headers, and read each of them into a pds dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696f0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = \"\"\"datetime  register_no    emp_no    trans_no    upc    description    trans_type    trans_subtype    trans_status    department\n",
    "quantity    Scale    cost    unitPrice    total    regPrice    altPrice    tax    taxexempt    foodstamp    wicable\n",
    "discount    memDiscount    discountable    discounttype    voided    percentDiscount    ItemQtty    volDiscType    volume\n",
    "VolSpecial    mixMatch    matched    memType    staff    numflag    itemstatus    tenderstatus    charflag    varflag\n",
    "batchHeaderID    local    organic    display    receipt    card_no    store    branch    match_id    trans_id\n",
    "\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc91e4d",
   "metadata": {},
   "source": [
    "# Cleaning Files and Uploading to Google Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b50829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first lets read in the zips and get a list of file names\n",
    "\n",
    "for zip in zip_files:\n",
    "    with ZipFile(\"Wedge_zips\\\\\" + zip, 'r') as zf :\n",
    "        files_in_zip = zf.namelist()\n",
    "        \n",
    "        #now I can open the files\n",
    "        for file_name in files_in_zip:\n",
    "            open_file = zf.open(file_name, 'r')\n",
    "            open_file = io.TextIOWrapper(open_file, encoding = \"utf-8\")\n",
    "            \n",
    "            #check open_file's delimeter\n",
    "            sniffer = csv.Sniffer().sniff(sample = open_file.readline())\n",
    "            \n",
    "            #check open_file for headers\n",
    "            for line in open_file:\n",
    "                \n",
    "                #now lets handle delimeters and headers while reading into a pd df\n",
    "                if line[0] == \"d\" or line[0:2] == '\"d':\n",
    "                    df = pd.read_csv(open_file, sep = sniffer.delimiter, encoding = \"utf-8\")\n",
    "                    \n",
    "                                        \n",
    "                else:\n",
    "                    df = pd.read_csv(open_file, sep = sniffer.delimiter, names = headers, encoding = \"utf-8\")\n",
    "                    \n",
    "            \n",
    "            for idx,column in enumerate(df):\n",
    "                df[column] = df[column].replace(np.nan, '', regex=True)\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'],format='%Y-%m-%d %H:%M:%S')\n",
    "                df['local'] = df['local'].fillna(0)\n",
    "                df['local'] = df['local'].replace('\\\\N', 0)\n",
    "                df['local'] = df['local'].astype(int)\n",
    "                df['altPrice'] = df['altPrice'].astype(str)\n",
    "                if df[column].dtypes == object:\n",
    "                    df=df.astype({column: 'str'})\n",
    "\n",
    "            \n",
    "            df = janitor.clean_names(df)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "            table_name = file_name.replace('.csv', '')\n",
    "            table_id = \".\".join([gbq_proj_id,dataset_id,table_name])\n",
    "            pandas_gbq.to_gbq(df,table_id,project_id=gbq_proj_id,if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2193536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0862e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
